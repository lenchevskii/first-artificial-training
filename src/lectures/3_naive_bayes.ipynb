{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# download(\"stopwords\")\n",
    "# download(\"wordnet\")\n",
    "\n",
    "path = \"/home/dmitry/Downloads/SMSSpamCollection\"\n",
    "df = pd.read_table(path, names=['label', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_df = df[\"text\"].apply(lambda x: x.lower()).apply(lambda y: \" \".join(re.findall(r\"\\w+\", y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(row):\n",
    "    row = re.findall(r\"\\w+\", row.lower())\n",
    "    row = map(lambda x: lemmatizer.lemmatize(x, \"v\"), row)\n",
    "    return \" \".join(filter(lambda x: x not in stopwords.words(\"english\"), row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df[\"text\"].apply(prepare_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_words = set()\n",
    "list(map(lambda x: uniq_words.update(x.split()), clean_df));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = len(uniq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for index, word in enumerate(uniq_words):\n",
    "    word2index[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conver_word2index(row):\n",
    "    return [word2index[i] for i in row.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_num = clean_df.apply(conver_word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_space(row):\n",
    "    while len(row) < 78:\n",
    "        row.append(space)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [5781, 2575, 4693, 1750, 6126, 1722, 4806, 652...\n",
       "1       [6887, 490, 2782, 6086, 4840, 5161, 7639, 7639...\n",
       "2       [1801, 1787, 3908, 5893, 6975, 5267, 1173, 178...\n",
       "3       [4840, 858, 2072, 398, 4567, 4840, 288, 7394, ...\n",
       "4       [5074, 2445, 5781, 2221, 7556, 6391, 3695, 763...\n",
       "                              ...                        \n",
       "5567    [2097, 3414, 4979, 3908, 2995, 4840, 4840, 526...\n",
       "5568    [3037, 3294, 5781, 6075, 1457, 2233, 7639, 763...\n",
       "5569    [4798, 5979, 540, 7639, 7639, 7639, 7639, 7639...\n",
       "5570    [2614, 4483, 1283, 2543, 4461, 160, 4950, 4096...\n",
       "5571    [5841, 655, 2497, 7639, 7639, 7639, 7639, 7639...\n",
       "Name: text, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df_num_equal = clean_df_num.apply(add_space)\n",
    "clean_df_num_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "x_train, x_test, y_train, y_test = train_test_split(clean_df_num_equal, df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([np.array(i) for i in x_train])\n",
    "x_test = np.array([np.array(i) for i in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8657573582196698"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BernoulliNB()\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backwords = df[\"text\"].apply(prepare_df)\n",
    "df_backwords_l = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_words = set()\n",
    "list(map(lambda x: uniq_words.update(x.split()), df_backwords));\n",
    "\n",
    "space = len(uniq_words)\n",
    "\n",
    "word2index = {}\n",
    "for index, word in enumerate(uniq_words, 1):\n",
    "    word2index[word] = index\n",
    "length_row = len(uniq_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_row(row):\n",
    "    out = np.zeros(length_row + 1)\n",
    "    row = row.split()\n",
    "    for word in row:\n",
    "        index = word2index[word]\n",
    "        out[index] = row.count(word)\n",
    "    return out\n",
    "df_bw_encoded = df_backwords.apply(encode_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df_backwords, df_backwords_l)\n",
    "x_train = np.array([np.array(i) for i in x_train])\n",
    "x_test = np.array([np.array(i) for i in x_test])\n",
    "\n",
    "# model = BernoulliNB()\n",
    "# model.fit(x_train, y_train)\n",
    "# model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit562c0e97d4464b2a83e5e3108965b493"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
